{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Points Analysis between Grab and Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from itertools import combinations\n",
    "from operator import contains\n",
    "\n",
    "import umap\n",
    "import miceforest as mf\n",
    "\n",
    "from copkmeans.cop_kmeans import cop_kmeans\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"data\")\n",
    "\n",
    "sensor_folder = os.path.join(data_folder, \"sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.read_excel(os.path.join(data_folder, \"grab.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict = {}\n",
    "\n",
    "for file in os.listdir(sensor_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        sensor_dict[file.split(\".\")[0]] = pd.read_excel(\n",
    "            os.path.join(sensor_folder, file)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(utils_folder, \"columns_types.json\")) as f:\n",
    "    column_types = json.load(f)\n",
    "\n",
    "metadata_columns = column_types[\"metadata_columns\"]\n",
    "features_columns = column_types[\"features_columns\"]\n",
    "targets_columns = column_types[\"targets_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [col for col in grab_df.columns if contains(col, \"label\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename grab columns\n",
    "feature_mapping = {\n",
    "    \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\": \"Free Chlorine (mg/L)\",\n",
    "    \"Colore (Cu)\": \"Color (CU)\",\n",
    "    \"Concentrazione ioni idrogeno (unità pH)\": \"pH\",\n",
    "    \"Conduttività a 20°C (µS/cm)\": \"Conductivity (uS/cm)\",\n",
    "    \"TOC - carbonio organico totale (mg/L di C)\": \"TOC (mg/L)\",\n",
    "    \"Temperatura (al prelievo) (°C)\": \"Temperature (°C)\",\n",
    "    \"Torbidità (NTu)\": \"Turbidity (NTU)\",\n",
    "    \"Nitrati (mg/L)\": \"Nitrate (mg/L)\",\n",
    "}\n",
    "\n",
    "targets_mapping = {\n",
    "    \"Bromodiclorometano (µg/L)\": \"Bromodichloromethane (µg/L)\",\n",
    "    \"Bromoformio (µg/L)\": \"Bromoform (µg/L)\",\n",
    "    \"Cloroformio (µg/L)\": \"Chloroform (µg/L)\",\n",
    "    \"Dibromoclorometano (µg/L)\": \"Dibromochloromethane (µg/L)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No THMs measurements before 2024, so we can drop all the rows before 2024\n",
    "grab_df = grab_df[grab_df[\"DateTime\"] > \"2024-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [\n",
    "            feature_mapping.values(),\n",
    "            [\n",
    "                \"N° Entries\",\n",
    "                \"N° Valid Samples\",\n",
    "                \"% Missing\",\n",
    "                \"N° < LOQ\",\n",
    "            ],\n",
    "        ]\n",
    "    ),\n",
    "    index=grab_df[\"Code\"].unique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        df = grab_df[grab_df[\"Code\"] == code][\n",
    "            [\"DateTime\", feature, feature + \"_label\"]\n",
    "        ].copy()\n",
    "\n",
    "        if df.dropna().shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "\n",
    "        start_date = df.dropna()[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.dropna()[\"DateTime\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        df = df[(df[\"DateTime\"] >= start_date) & (df[\"DateTime\"] <= end_date)]\n",
    "\n",
    "        missing_values = df[df[feature + \"_label\"].isna()].shape[0] / df.shape[0] * 100\n",
    "\n",
    "        feature_df.loc[code, (feature, \"N° Entries\")] = df.shape[0]\n",
    "\n",
    "        feature_df.loc[code, (feature, \"% Missing\")] = round(missing_values, 2)\n",
    "\n",
    "        feature_df.loc[code, (feature, \"N° < LOQ\")] = df[\n",
    "            df[feature + \"_label\"] == \"Less than\"\n",
    "        ].shape[0]\n",
    "\n",
    "        valid_df = df[df[feature + \"_label\"] == \"Normal\"]\n",
    "        loq_df = df[df[feature + \"_label\"] == \"Less than\"]\n",
    "\n",
    "        feature_df.loc[code, (feature, \"N° Valid Samples\")] = valid_df.shape[0]\n",
    "        feature_df.loc[code, (feature, \"N° < LOQ\")] = loq_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the indexes\n",
    "feature_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the first level of the columns and maintain the order of the second level\n",
    "feature_df = feature_df.sort_index(\n",
    "    axis=1, level=0, sort_remaining=False, key=lambda x: x.str.lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [\n",
    "            targets_mapping.values(),\n",
    "            [\n",
    "                \"N° Entries\",\n",
    "                \"N° Valid Samples\",\n",
    "                \"% Missing\",\n",
    "                \"N° < LOQ\",\n",
    "            ],\n",
    "        ]\n",
    "    ),\n",
    "    index=grab_df[\"Code\"].unique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    for target in targets_mapping.values():\n",
    "        df = grab_df[grab_df[\"Code\"] == code][\n",
    "            [\"DateTime\", target, target + \"_label\"]\n",
    "        ].copy()\n",
    "\n",
    "        if df.dropna().shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "\n",
    "        start_date = df.dropna()[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.dropna()[\"DateTime\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        df = df[(df[\"DateTime\"] >= start_date) & (df[\"DateTime\"] <= end_date)]\n",
    "\n",
    "        missing_values = df[df[target + \"_label\"].isna()].shape[0] / df.shape[0] * 100\n",
    "\n",
    "        targets_df.loc[code, (target, \"N° Entries\")] = df.shape[0]\n",
    "\n",
    "        valid_df = df[df[target + \"_label\"] == \"Normal\"]\n",
    "        loq_df = df[df[target + \"_label\"] == \"Less than\"]\n",
    "\n",
    "        targets_df.loc[code, (target, \"% Missing\")] = round(missing_values, 2)\n",
    "\n",
    "        targets_df.loc[code, (target, \"N° Valid Samples\")] = valid_df.shape[0]\n",
    "        targets_df.loc[code, (target, \"N° < LOQ\")] = loq_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = targets_df.sort_index(\n",
    "    axis=1, level=0, sort_remaining=False, key=lambda x: x.str.lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix Conductivity name\n",
    "for sensor in sensor_dict:\n",
    "    sensor_dict[sensor].rename(\n",
    "        columns={\"Conductivity (μS/cm)\": \"Conductivity (uS/cm)\"}, inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = sensor_dict[\"Berna\"].columns.difference([\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [\n",
    "            sensor_columns,\n",
    "            [\"N° Data\", \"N° Missing\", \"Mean\", \"Std\", \"Start Date\", \"End Date\"],\n",
    "        ]\n",
    "    ),\n",
    "    index=list(sensor_dict.keys()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in sensor_dict.keys():\n",
    "    for column in sensor_columns:\n",
    "        if sensor == \"Berna\" and column == \"Turbidity (FTU)\":\n",
    "            df = sensor_dict[sensor].copy()\n",
    "            # remove rows with Turbidity > 2\n",
    "            df = df[df[\"Turbidity (FTU)\"] <= 2]\n",
    "\n",
    "            sensors_df.loc[sensor, (column, \"N° Data\")] = df[column].count()\n",
    "            sensors_df.loc[sensor, (column, \"N° Missing\")] = df[column].isna().sum()\n",
    "            sensors_df.loc[sensor, (column, \"Mean\")] = df[column].mean()\n",
    "            sensors_df.loc[sensor, (column, \"Std\")] = df[column].std()\n",
    "            continue\n",
    "\n",
    "        sensors_df.loc[sensor, (column, \"N° Data\")] = sensor_dict[sensor][\n",
    "            column\n",
    "        ].count()\n",
    "        sensors_df.loc[sensor, (column, \"N° Missing\")] = (\n",
    "            sensor_dict[sensor][column].isna().sum()\n",
    "        )\n",
    "        sensors_df.loc[sensor, (column, \"Mean\")] = sensor_dict[sensor][column].mean()\n",
    "        sensors_df.loc[sensor, (column, \"Std\")] = sensor_dict[sensor][column].std()\n",
    "\n",
    "        start_date = sensor_dict[sensor][\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = sensor_dict[sensor][\"DateTime\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        sensors_df.loc[sensor, (column, \"Start Date\")] = start_date\n",
    "        sensors_df.loc[sensor, (column, \"End Date\")] = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plot Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot pair grid for grab features\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "sns.pairplot(grab_df, vars=feature_mapping.values(), hue=\"Code\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "# plot the time series of the sensors and the grab data\n",
    "\n",
    "\n",
    "n_hours = 3\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "\n",
    "        g_df = g_df[[\"DateTime\", feature]].copy()\n",
    "        g_df.dropna(inplace=True)\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        start_date = s_df[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        g_df = g_df[g_df[\"DateTime\"] >= start_date]\n",
    "\n",
    "        # moving average on sensor data\n",
    "\n",
    "        ma_s_df = s_df.copy()\n",
    "\n",
    "        ma_s_df.set_index(\"DateTime\", inplace=True)\n",
    "        ma_s_df = ma_s_df.rolling(window=4 * n_hours).mean()\n",
    "\n",
    "        std = g_df[feature].std()\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=s_df[\"DateTime\"], y=s_df[feature], mode=\"lines\", name=\"Sensor\")\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ma_s_df.index,\n",
    "                y=ma_s_df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=\"Sensor MA\",\n",
    "                line=dict(color=\"green\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=g_df[\"DateTime\"],\n",
    "                y=g_df[feature],\n",
    "                mode=\"markers\",\n",
    "                name=\"Grab\",\n",
    "                marker=dict(size=12, color=\"red\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # add the std to each point of the grab data\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            xaxis_title=\"DateTime\",\n",
    "            yaxis_title=feature,\n",
    "            # put legend inside the plot\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1,\n",
    "            ),\n",
    "            margin=dict(l=0, r=0, t=0, b=0),\n",
    "            hovermode=\"closest\",\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplot Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    if month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    if month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    if month in [9, 10, 11]:\n",
    "        return \"Autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script --false --no-raise-error\n",
    "\n",
    "# plot the box plot of grab data and sensor data\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        s_df = s_df.rolling(window=4 * n_hours).mean()\n",
    "\n",
    "        sensor_start_date = s_df.index.min().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "\n",
    "        before_g_df = g_df[g_df[\"DateTime\"] < sensor_start_date]\n",
    "        after_g_df = g_df[g_df[\"DateTime\"] >= sensor_start_date]\n",
    "\n",
    "        valid_g_df = g_df[g_df[feature + \"_label\"] == \"Normal\"]\n",
    "        loq_g_df = g_df[g_df[feature + \"_label\"] == \"Less than\"]\n",
    "\n",
    "        valid_before_g_df = valid_g_df[valid_g_df[\"DateTime\"] < sensor_start_date]\n",
    "        valid_after_g_df = valid_g_df[valid_g_df[\"DateTime\"] >= sensor_start_date]\n",
    "\n",
    "        loq_before_g_df = loq_g_df[loq_g_df[\"DateTime\"] < sensor_start_date]\n",
    "        loq_after_g_df = loq_g_df[loq_g_df[\"DateTime\"] >= sensor_start_date]\n",
    "\n",
    "        # divide before and after into seasons\n",
    "        valid_before_g_df[\"Season\"] = valid_before_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "        valid_after_g_df[\"Season\"] = valid_after_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "\n",
    "        loq_before_g_df[\"Season\"] = loq_before_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "        loq_after_g_df[\"Season\"] = loq_after_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=3,\n",
    "            cols=1,\n",
    "            specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}], [{\"type\": \"table\"}]],\n",
    "            subplot_titles=(\n",
    "                \"\",\n",
    "                f\"Grab Samples Before {sensor_start_date}\",\n",
    "                f\"Grab Samples After {sensor_start_date}\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=valid_before_g_df[feature],\n",
    "                name=f\"Valid Old Grab<br>N° Points: {valid_before_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=loq_before_g_df[feature],\n",
    "                name=f\"LOQ Old Grab<br>N° Points: {loq_before_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=before_g_df[feature],\n",
    "                name=f\"Overall Old Grab<br>N° Points: {before_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=valid_after_g_df[feature],\n",
    "                name=f\"Valid New Grab<br>N° Points: {valid_after_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=loq_after_g_df[feature],\n",
    "                name=f\"LOQ New Grab<br>N° Points: {loq_after_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=after_g_df[feature],\n",
    "                name=f\"Overall New Grab<br>N° Points: {after_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(go.Box(y=s_df[feature], name=\"Sensor\"), row=1, col=1)\n",
    "\n",
    "        # divide by season for both old and new grab data\n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=[\n",
    "                        \"Season Valid\",\n",
    "                        \"N° Points Valid\",\n",
    "                        \"Mean Valid\",\n",
    "                        \"Std Valid\",\n",
    "                        \"Season LOQ\",\n",
    "                        \"N° Points LOQ\",\n",
    "                        \"Mean LOQ\",\n",
    "                        \"Std LOQ\",\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        valid_before_g_df.groupby(\"Season\").size().index,\n",
    "                        valid_before_g_df.groupby(\"Season\")[feature].count().values,\n",
    "                        valid_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        valid_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .std()\n",
    "                        .values.round(2),\n",
    "                        loq_before_g_df.groupby(\"Season\").size().index,\n",
    "                        loq_before_g_df.groupby(\"Season\")[feature].count().values,\n",
    "                        loq_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        loq_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .std()\n",
    "                        .values.round(2),\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=[\n",
    "                        \"Season Valid\",\n",
    "                        \"N° Points Valid\",\n",
    "                        \"Mean Valid\",\n",
    "                        \"Std Valid\",\n",
    "                        \"Season LOQ\",\n",
    "                        \"N° Points LOQ\",\n",
    "                        \"Mean LOQ\",\n",
    "                        \"Std LOQ\",\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        valid_after_g_df.groupby(\"Season\").size().index,\n",
    "                        valid_after_g_df.groupby(\"Season\")[feature].count(),\n",
    "                        valid_after_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        valid_after_g_df.groupby(\"Season\")[feature]\n",
    "                        .std()\n",
    "                        .values.round(2),\n",
    "                        loq_after_g_df.groupby(\"Season\").size().index,\n",
    "                        loq_after_g_df.groupby(\"Season\")[feature].count(),\n",
    "                        loq_after_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        loq_after_g_df.groupby(\"Season\")[feature].std().values.round(2),\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "            ),\n",
    "            row=3,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            yaxis_title=feature,\n",
    "        )\n",
    "\n",
    "        fig.add_annotation(\n",
    "            dict(\n",
    "                x=-0.022,\n",
    "                y=1.07,\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                showarrow=False,\n",
    "                text=f\"Grab Samples divided by date {sensor_start_date}\",\n",
    "                font=dict(size=12, color=\"gray\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bland-Altman Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# With all the supply points together\n",
    "\n",
    "\n",
    "total_g_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "total_s_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "\n",
    "        # if code == \"Berna\" and feature == \"Free Chlorine (mg/L)\":\n",
    "        #     pass\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        # 2 hours moving average\n",
    "        s_df = s_df.rolling(window=4 * 2).mean()\n",
    "\n",
    "        # fix the date of the sensor data to have a frequency of 15 minutes for easier comparison and interpolate to not have nan value\n",
    "        s_df = s_df.resample(\"15min\").mean().interpolate(method=\"time\")\n",
    "\n",
    "        sensor_start_date = s_df.index.dropna().min().strftime(\"%Y-%m-%d\")\n",
    "        sensor_end_date = s_df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        g_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        g_df = g_df[(g_df.index >= sensor_start_date) & (g_df.index <= sensor_end_date)]\n",
    "\n",
    "        g_df = g_df[feature]\n",
    "        g_df.dropna(inplace=True)\n",
    "\n",
    "        # keep only the sensor values that have the date in the grab data and the hour is between 9 and 11\n",
    "\n",
    "        dates = pd.Series(s_df.index.date, index=s_df.index).isin(g_df.index.date)\n",
    "        dates = dates[dates.values]\n",
    "\n",
    "        s_df = s_df.loc[dates.index]\n",
    "        s_df = s_df[\n",
    "            (s_df.index.hour == 10)\n",
    "            & (s_df.index.minute >= 0)\n",
    "            & (s_df.index.minute <= 14)\n",
    "        ]\n",
    "\n",
    "        # if there is more than one value for the same date, take the mean\n",
    "        s_df = s_df.groupby(s_df.index.date).mean()\n",
    "\n",
    "        total_g_df = pd.concat(\n",
    "            [\n",
    "                total_g_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Code\": code,\n",
    "                        \"DateTime\": g_df.index,\n",
    "                        \"Feature\": feature,\n",
    "                        \"Value\": g_df.values,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        total_s_df = pd.concat(\n",
    "            [\n",
    "                total_s_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Code\": code,\n",
    "                        \"DateTime\": s_df.index,\n",
    "                        \"Feature\": feature,\n",
    "                        \"Value\": s_df[feature].values,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "\n",
    "    g_df = total_g_df[total_g_df[\"Feature\"] == feature]\n",
    "    s_df = total_s_df[total_s_df[\"Feature\"] == feature]\n",
    "\n",
    "    g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "    s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "\n",
    "    df = pd.merge(g_df, s_df, on=[\"Code\", \"DateTime\"], suffixes=(\"_Grab\", \"_Sensor\"))\n",
    "    df[\"Difference\"] = df[\"Value_Grab\"] - df[\"Value_Sensor\"]\n",
    "    df[\"Mean\"] = (df[\"Value_Grab\"] + df[\"Value_Sensor\"]) / 2\n",
    "\n",
    "    difference_mean = np.mean(df[\"Difference\"].values)\n",
    "    difference_std = np.std(df[\"Difference\"].values)\n",
    "    std_error = difference_std / np.sqrt(g_df.shape[0])\n",
    "\n",
    "    ci_difference_mean = 1.96 * std_error\n",
    "\n",
    "    f, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    sns.scatterplot(data=df, x=\"Mean\", y=\"Difference\", hue=\"Code\", ax=ax, s=100)\n",
    "\n",
    "    ax.axhline(y=difference_mean, color=\"green\", linestyle=\"--\", label=\"Mean\")\n",
    "\n",
    "    if feature == \"Conductivity (uS/cm)\":\n",
    "        ax.text(\n",
    "            x=712, y=difference_mean, s=f\"Mean: {difference_mean:.2f}\", color=\"green\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        ax.text(\n",
    "            x=df[\"Mean\"].quantile(0.9),\n",
    "            y=difference_mean + std_error,\n",
    "            s=f\"Mean: {difference_mean:.2f}\",\n",
    "            color=\"green\",\n",
    "        )\n",
    "\n",
    "    ax.axhline(\n",
    "        y=difference_mean + 1.96 * difference_std,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"1.96 * Std\",\n",
    "    )\n",
    "    # add text over the horizontal line\n",
    "\n",
    "    if feature == \"Conductivity (uS/cm)\":\n",
    "        ax.text(\n",
    "            x=712,\n",
    "            y=difference_mean + 1.96 * difference_std,\n",
    "            s=f\"+ 1.96 * Std\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        ax.text(\n",
    "            x=df[\"Mean\"].quantile(0.9),\n",
    "            y=difference_mean + 1.96 * difference_std + std_error,\n",
    "            s=f\"+ 1.96 * Std\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    ax.axhline(\n",
    "        y=difference_mean - 1.96 * difference_std,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"-1.96 * Std\",\n",
    "    )\n",
    "    # add text over the horizontal line\n",
    "\n",
    "    if feature == \"Conductivity (uS/cm)\":\n",
    "        ax.text(\n",
    "            x=712,\n",
    "            y=difference_mean - 1.96 * difference_std,\n",
    "            s=f\"-1.96 * Std\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        ax.text(\n",
    "            x=df[\"Mean\"].quantile(0.9),\n",
    "            y=difference_mean - 1.96 * difference_std + std_error,\n",
    "            s=f\"-1.96 * Std\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    plt.annotate(\n",
    "        f\"Std error: {std_error:.2f}\\nDifference mean CI: {difference_mean:.2f} ± {ci_difference_mean:.2f}\",\n",
    "        xy=(0.6, 0.94),\n",
    "        xycoords=\"axes fraction\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"),\n",
    "        color=\"green\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{feature} - Bland-Altman Plot\")\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "    # increment the font size\n",
    "    for item in (\n",
    "        [ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "        + ax.get_xticklabels()\n",
    "        + ax.get_yticklabels()\n",
    "    ):\n",
    "        item.set_fontsize(16)\n",
    "\n",
    "    for item in ax.get_legend().get_texts():\n",
    "        item.set_fontsize(14)\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "# With all the supply points together and DateTime on the x axis\n",
    "\n",
    "total_g_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "total_s_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "\n",
    "        # if code == \"Berna\" and feature == \"Free Chlorine (mg/L)\":\n",
    "        #     pass\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        # 2 hours moving average\n",
    "        s_df = s_df.rolling(window=4 * 2).mean()\n",
    "\n",
    "        # fix the date of the sensor data to have a frequency of 15 minutes for easier comparison and interpolate to not have nan value\n",
    "        s_df = s_df.resample(\"15min\").mean().interpolate(method=\"time\")\n",
    "\n",
    "        sensor_start_date = s_df.index.dropna().min().strftime(\"%Y-%m-%d\")\n",
    "        sensor_end_date = s_df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        g_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        g_df = g_df[(g_df.index >= sensor_start_date) & (g_df.index <= sensor_end_date)]\n",
    "\n",
    "        g_df = g_df[feature]\n",
    "        g_df.dropna(inplace=True)\n",
    "\n",
    "        # keep only the sensor values that have the date in the grab data and the hour is between 9 and 11\n",
    "\n",
    "        dates = pd.Series(s_df.index.date, index=s_df.index).isin(g_df.index.date)\n",
    "        dates = dates[dates.values]\n",
    "\n",
    "        s_df = s_df.loc[dates.index]\n",
    "        s_df = s_df[\n",
    "            (s_df.index.hour == 10)\n",
    "            & (s_df.index.minute >= 0)\n",
    "            & (s_df.index.minute <= 14)\n",
    "        ]\n",
    "\n",
    "        # if there is more than one value for the same date, take the mean\n",
    "        s_df = s_df.groupby(s_df.index.date).mean()\n",
    "\n",
    "        total_g_df = pd.concat(\n",
    "            [\n",
    "                total_g_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Code\": code,\n",
    "                        \"DateTime\": g_df.index,\n",
    "                        \"Feature\": feature,\n",
    "                        \"Value\": g_df.values,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        total_s_df = pd.concat(\n",
    "            [\n",
    "                total_s_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Code\": code,\n",
    "                        \"DateTime\": s_df.index,\n",
    "                        \"Feature\": feature,\n",
    "                        \"Value\": s_df[feature].values,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "\n",
    "    g_df = total_g_df[total_g_df[\"Feature\"] == feature]\n",
    "    s_df = total_s_df[total_s_df[\"Feature\"] == feature]\n",
    "\n",
    "    g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "    s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "\n",
    "    df = pd.merge(g_df, s_df, on=[\"Code\", \"DateTime\"], suffixes=(\"_Grab\", \"_Sensor\"))\n",
    "    df[\"Difference\"] = df[\"Value_Grab\"] - df[\"Value_Sensor\"]\n",
    "    df[\"Mean\"] = (df[\"Value_Grab\"] + df[\"Value_Sensor\"]) / 2\n",
    "\n",
    "    difference_mean = np.mean(df[\"Difference\"].values)\n",
    "    difference_std = np.std(df[\"Difference\"].values)\n",
    "    std_error = difference_std / np.sqrt(g_df.shape[0])\n",
    "\n",
    "    ci_difference_mean = 1.96 * std_error\n",
    "\n",
    "    f, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    sns.scatterplot(data=df, x=\"DateTime\", y=\"Difference\", hue=\"Code\", ax=ax, s=100)\n",
    "\n",
    "    ax.axhline(y=difference_mean, color=\"green\", linestyle=\"--\", label=\"Mean\")\n",
    "\n",
    "    ax.text(\n",
    "        x=pd.Timestamp(\"2024-08-15\"),\n",
    "        y=difference_mean + std_error,\n",
    "        s=f\"Mean: {difference_mean:.2f}\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "\n",
    "    ax.axhline(\n",
    "        y=difference_mean + 1.96 * difference_std,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"1.96 * Std\",\n",
    "    )\n",
    "    # add text over the horizontal line\n",
    "    ax.text(\n",
    "        x=pd.Timestamp(\"2024-08-15\"),\n",
    "        y=difference_mean + 1.96 * difference_std + std_error,\n",
    "        s=f\"+ 1.96 * Std\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    ax.axhline(\n",
    "        y=difference_mean - 1.96 * difference_std,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"-1.96 * Std\",\n",
    "    )\n",
    "    # add text over the horizontal line\n",
    "    ax.text(\n",
    "        x=pd.Timestamp(\"2024-08-15\"),\n",
    "        y=difference_mean - 1.96 * difference_std + std_error,\n",
    "        s=f\"-1.96 * Std\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    plt.annotate(\n",
    "        f\"Std error: {std_error:.2f}\\nDifference mean CI: {difference_mean:.2f} ± {ci_difference_mean:.2f}\",\n",
    "        xy=(0.05, 0.9),\n",
    "        xycoords=\"axes fraction\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"),\n",
    "        color=\"green\",\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{feature} - Bland-Altman Plot\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Measurement Time between Supply Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the timestamps are the same for each code\n",
    "\n",
    "common_dates = pd.Series()\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "\n",
    "    code_df = grab_df[grab_df[\"Code\"] == code][\"DateTime\"].copy()\n",
    "\n",
    "    if common_dates.empty:\n",
    "        common_dates = code_df\n",
    "\n",
    "    else:\n",
    "        common_dates = pd.Series(list(set(common_dates).intersection(set(code_df))))\n",
    "\n",
    "common_dates = common_dates.sort_values()\n",
    "\n",
    "common_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in list(feature_mapping.values()):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for code in grab_df[\"Code\"].unique():\n",
    "        code_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=code_df[\"DateTime\"], y=code_df[feature], mode=\"markers\", name=code\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(title=feature, xaxis_title=\"DateTime\", yaxis_title=feature)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation\n",
    "\n",
    "Value points with the label 'Less than' are imputed with LOQ/2, while value points with label 'NaN' are imputed with MICE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_loq(row, column):\n",
    "    return row[column] if row[column + \"_label\"] != \"Less than\" else row[column] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [col for col in grab_df.columns if \"label\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grab_df.copy()\n",
    "\n",
    "for column in grab_df.columns.difference([\"Code\", \"DateTime\"] + label_columns):\n",
    "    df[column] = df.apply(lambda row: replace_loq(row, column), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[list(feature_mapping.values()) + [\"DateTime\"] + [\"Code\"]]\n",
    "\n",
    "# convert datetime column to float\n",
    "df[\"DateTime\"] = pd.to_numeric(df[\"DateTime\"])\n",
    "df[\"Code\"] = df[\"Code\"].astype(\"category\")\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    if df[df[\"Code\"] == code].isnull().all(axis=1).sum() > 0:  # Rows with all NaN\n",
    "        print(\n",
    "            f\"{code} has {df[df['Code'] == code].isnull().all(axis=1).sum()} rows with all NaN\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    if df[df[\"Code\"] == code][\"Turbidity (NTU)\"].isnull().sum() > 0:\n",
    "        print(\n",
    "            f'{code} has {df[df[\"Code\"] == code][\"Turbidity (NTU)\"].isnull().sum()} NaN values for Turbidity (NTU)'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    if df[df[\"Code\"] == code].isnull().all(axis=0).sum() > 0:  # Columns with all NaN\n",
    "        print(\n",
    "            f'{code} has {df[df[\"Code\"] == code].isnull().all(axis=0).sum()} columns with all NaN'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the turbidity since it has a lot of missing values and the sensors are not well calibrated\n",
    "df.drop(columns=[\"Turbidity (NTU)\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapping.pop(\"Torbidità (NTu)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MICE imputation\n",
    "\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=df,\n",
    "    variable_schema=df.columns.difference([\"DateTime\", \"Code\"]).to_list(),\n",
    "    random_state=seed,\n",
    "    mean_match_strategy=\"shap\",\n",
    ")\n",
    "\n",
    "kernel.mice(4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset[\"DateTime\"] = pd.to_datetime(completed_dataset[\"DateTime\"])\n",
    "completed_dataset[\"Code\"] = completed_dataset[\"Code\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bland-Altman Imputed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# With all the supply points together\n",
    "\n",
    "total_g_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "total_s_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = completed_dataset[completed_dataset[\"Code\"] == code].copy()\n",
    "\n",
    "        # if code == \"Berna\" and feature == \"Free Chlorine (mg/L)\":\n",
    "        #     pass\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        # 2 hours moving average\n",
    "        s_df = s_df.rolling(window=4 * 2).mean()\n",
    "\n",
    "        # fix the date of the sensor data to have a frequency of 15 minutes for easier comparison and interpolate to not have nan value\n",
    "        s_df = s_df.resample(\"15min\").mean().interpolate(method=\"time\")\n",
    "\n",
    "        sensor_start_date = s_df.index.dropna().min().strftime(\"%Y-%m-%d\")\n",
    "        sensor_end_date = s_df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        g_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        g_df = g_df[(g_df.index >= sensor_start_date) & (g_df.index <= sensor_end_date)]\n",
    "\n",
    "        g_df = g_df[feature]\n",
    "        g_df.dropna(inplace=True)\n",
    "\n",
    "        # keep only the sensor values that have the date in the grab data and the hour is between 9 and 11\n",
    "\n",
    "        dates = pd.Series(s_df.index.date, index=s_df.index).isin(g_df.index.date)\n",
    "        dates = dates[dates.values]\n",
    "\n",
    "        s_df = s_df.loc[dates.index]\n",
    "        s_df = s_df[\n",
    "            (s_df.index.hour == 10)\n",
    "            & (s_df.index.minute >= 0)\n",
    "            & (s_df.index.minute <= 14)\n",
    "        ]\n",
    "\n",
    "        # if there is more than one value for the same date, take the mean\n",
    "        s_df = s_df.groupby(s_df.index.date).mean()\n",
    "\n",
    "        total_g_df = pd.concat(\n",
    "            [\n",
    "                total_g_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Code\": code,\n",
    "                        \"DateTime\": g_df.index,\n",
    "                        \"Feature\": feature,\n",
    "                        \"Value\": g_df.values,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        total_s_df = pd.concat(\n",
    "            [\n",
    "                total_s_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Code\": code,\n",
    "                        \"DateTime\": s_df.index,\n",
    "                        \"Feature\": feature,\n",
    "                        \"Value\": s_df[feature].values,\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "\n",
    "    g_df = total_g_df[total_g_df[\"Feature\"] == feature]\n",
    "    s_df = total_s_df[total_s_df[\"Feature\"] == feature]\n",
    "\n",
    "    g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "    s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "\n",
    "    df = pd.merge(g_df, s_df, on=[\"Code\", \"DateTime\"], suffixes=(\"_Grab\", \"_Sensor\"))\n",
    "    df[\"Difference\"] = df[\"Value_Grab\"] - df[\"Value_Sensor\"]\n",
    "    df[\"Mean\"] = (df[\"Value_Grab\"] + df[\"Value_Sensor\"]) / 2\n",
    "\n",
    "    difference_mean = np.mean(df[\"Difference\"].values)\n",
    "    difference_std = np.std(df[\"Difference\"].values)\n",
    "    std_error = difference_std / np.sqrt(g_df.shape[0])\n",
    "\n",
    "    ci_difference_mean = 1.96 * std_error\n",
    "\n",
    "    f, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    sns.scatterplot(data=df, x=\"Mean\", y=\"Difference\", hue=\"Code\", ax=ax, s=100)\n",
    "\n",
    "    ax.axhline(y=difference_mean, color=\"green\", linestyle=\"--\", label=\"Mean\")\n",
    "\n",
    "    ax.text(\n",
    "        x=df[\"Mean\"].quantile(0.97),\n",
    "        y=difference_mean + std_error,\n",
    "        s=f\"Mean: {difference_mean:.2f}\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "\n",
    "    ax.axhline(\n",
    "        y=difference_mean + 1.96 * difference_std,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"1.96 * Std\",\n",
    "    )\n",
    "    # add text over the horizontal line\n",
    "    ax.text(\n",
    "        x=df[\"Mean\"].quantile(0.97),\n",
    "        y=difference_mean + 1.96 * difference_std + std_error,\n",
    "        s=f\"+ 1.96 * Std\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    ax.axhline(\n",
    "        y=difference_mean - 1.96 * difference_std,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"-1.96 * Std\",\n",
    "    )\n",
    "    # add text over the horizontal line\n",
    "    ax.text(\n",
    "        x=df[\"Mean\"].quantile(0.97),\n",
    "        y=difference_mean - 1.96 * difference_std + std_error,\n",
    "        s=f\"-1.96 * Std\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    plt.annotate(\n",
    "        f\"Std error: {std_error:.2f}\\nDifference mean CI: {difference_mean:.2f} ± {ci_difference_mean:.2f}\",\n",
    "        xy=(0.05, 0.9),\n",
    "        xycoords=\"axes fraction\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"),\n",
    "        color=\"green\",\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{feature} - Bland-Altman Plot\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_codes = grab_df[\"Code\"].unique()\n",
    "\n",
    "code_mapping = {code: i for i, code in enumerate(house_codes)}\n",
    "df = completed_dataset[[\"Code\"] + list(feature_mapping.values())].copy()\n",
    "\n",
    "df[\"Code\"] = df[\"Code\"].map(code_mapping)\n",
    "df[\"Code\"] = df[\"Code\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert the code mapping\n",
    "# df['Code'] = df['Code'].map({v: k for k, v in code_mapping.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_neighbors=10, random_state=42).fit(\n",
    "    df[[\"Code\"] + list(feature_mapping.values())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(mapper, labels=df[\"Code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "We are going to use COP-KMeans on the joint distribution (features + TTHMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thms_columns = [\n",
    "    \"Bromodichloromethane (µg/L)\",\n",
    "    \"Bromoform (µg/L)\",\n",
    "    \"Chloroform (µg/L)\",\n",
    "    \"Dibromochloromethane (µg/L)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = completed_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tthms_df = grab_df[thms_columns].copy()\n",
    "tthms_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the thms columns\n",
    "df[\"TTHMs\"] = tthms_df.sum(axis=1, min_count=len(thms_columns)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_columns = list(feature_mapping.values()) + [\"TTHMs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_codes = df[\"Code\"].unique()\n",
    "\n",
    "code_mapping = {code: i for i, code in enumerate(house_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Code\"] = df[\"Code\"].map(code_mapping)\n",
    "df[\"Code\"] = df[\"Code\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df.copy()\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataframe[joint_columns] = pd.DataFrame(\n",
    "    scaler.fit_transform(dataframe[joint_columns]), columns=joint_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe[\"Code\"].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe[\"Code\"] == code].index, 2))\n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[joint_columns].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "variances = []\n",
    "sil_scores = []\n",
    "\n",
    "total_points = dataframe.shape[0]\n",
    "\n",
    "variance = np.var(dataframe[joint_columns], axis=0).mean()\n",
    "variances.append(variance)\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    clusters, centers = cop_kmeans(np_df, n_cluster, ml=must_link)\n",
    "\n",
    "    sil_score = silhouette_score(np_df, clusters)\n",
    "\n",
    "    dataframe[\"Cluster\"] = clusters\n",
    "\n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in dataframe[\"Cluster\"].unique():\n",
    "        cluster_df = dataframe[dataframe[\"Cluster\"] == cluster].copy()\n",
    "        variance += (\n",
    "            np.var(cluster_df[joint_columns], axis=0).mean()\n",
    "            * cluster_df.shape[0]\n",
    "            / total_points\n",
    "        )\n",
    "\n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster\n",
    "\n",
    "    variances.append(variance)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "    dataframe.drop(columns=[\"Cluster\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the scores\n",
    "variances = np.array(variances)\n",
    "sil_scores = np.array(sil_scores)\n",
    "\n",
    "variances = scaler.fit_transform(variances.reshape(-1, 1)).flatten()\n",
    "sil_scores = scaler.fit_transform(sil_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "plt.plot(range(1, 10), variances, label=\"Weighted Average Variance\")\n",
    "plt.plot(range(2, 10), sil_scores, label=\"Silhouette Score\")\n",
    "\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"(Normalized) Scores\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a good number of clusters is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "n_clusters = 3\n",
    "\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe[\"Code\"].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe[\"Code\"] == code].index, 2))\n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[joint_columns].to_numpy()\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"Cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in dataframe[\"Cluster\"].unique():\n",
    "    print(f\"Cluster {cluster}\")\n",
    "    codes = dataframe[dataframe[\"Cluster\"] == cluster][\"Code\"].unique().tolist()\n",
    "    # get the key from the value\n",
    "    codes = [k for k, v in code_mapping.items() if v in codes]\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap the code to the original code\n",
    "dataframe[\"Code\"] = dataframe[\"Code\"].map({v: k for k, v in code_mapping.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Code\"] = dataframe[\"Code\"]\n",
    "df[\"Cluster\"] = dataframe[\"Cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(os.path.join(data_folder, \"modelling_grab.xlsx\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-t4-2-cs2-supply-points-VbhzC-WI-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
