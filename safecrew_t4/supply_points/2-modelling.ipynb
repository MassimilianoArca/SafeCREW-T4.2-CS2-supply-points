{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling of supply points for soft-sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"data\")\n",
    "\n",
    "sensor_folder = os.path.join(data_folder, \"sensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.read_excel(os.path.join(data_folder, \"modelling_grab.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = {}\n",
    "for cluster in grab_df['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = grab_df[grab_df['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    codes_dict[cluster] = codes\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = grab_df[grab_df['Cluster'] == 0].copy()\n",
    "cluster_1_df = grab_df[grab_df['Cluster'] == 1].copy()\n",
    "cluster_2_df = grab_df[grab_df['Cluster'] == 2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df.drop(columns=['Code', 'DateTime', 'Cluster'], inplace=True)\n",
    "cluster_1_df.drop(columns=['Code', 'DateTime', 'Cluster'], inplace=True)\n",
    "cluster_2_df.drop(columns=['Code', 'DateTime', 'Cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {\n",
    "    'cluster_0': cluster_0_df,\n",
    "    'cluster_1': cluster_1_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict = {}\n",
    "\n",
    "for file in os.listdir(sensor_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        sensor_dict[file.split(\".\")[0]] = pd.read_excel(\n",
    "            os.path.join(sensor_folder, file)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_sensor_dict = {}\n",
    "cluster_1_sensor_dict = {}\n",
    "cluster_2_sensor_dict = {}\n",
    "\n",
    "for sensor_name in sensor_dict.keys():\n",
    "    if sensor_name in codes_dict[0]:\n",
    "        cluster_0_sensor_dict[sensor_name] = sensor_dict[sensor_name]\n",
    "    elif sensor_name in codes_dict[1]:\n",
    "        cluster_1_sensor_dict[sensor_name] = sensor_dict[sensor_name]\n",
    "    elif sensor_name in codes_dict[2]:\n",
    "        cluster_2_sensor_dict[sensor_name] = sensor_dict[sensor_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for supply_point_name in cluster_0_sensor_dict.keys():\n",
    "    sensor_df = cluster_0_sensor_dict[supply_point_name]\n",
    "    \n",
    "    # for the moment remove the uva since it is not present in the grab data\n",
    "    if 'UVA254 (1/m)' in sensor_df.columns:\n",
    "        sensor_df.drop(columns=['UVA254 (1/m)'], inplace=True)\n",
    "    \n",
    "    # remove turbidity since it is not present in the grab data\n",
    "    if 'Turbidity (NTU)' in sensor_df.columns:\n",
    "        sensor_df.drop(columns=['Turbidity (NTU)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_name in clusters.keys():\n",
    "    df = clusters[cluster_name]\n",
    "    X, y = df.drop(columns=['TTHMs']), df['TTHMs']\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    clusters[cluster_name] = X_scaled, y\n",
    "    \n",
    "    # put in the same order the columns of the sensors\n",
    "    X_columns = X.columns.tolist()    \n",
    "    \n",
    "    # Sensors \n",
    "    if cluster_name == 'cluster_0':\n",
    "        for sensor_name in cluster_0_sensor_dict.keys():\n",
    "            sensor_df = cluster_0_sensor_dict[sensor_name].copy()\n",
    "            \n",
    "            datetime_col = sensor_df['DateTime']\n",
    "            sensor_df.drop(columns=['DateTime'], inplace=True)\n",
    "            \n",
    "            X_sensor = pd.DataFrame(scaler.fit_transform(sensor_df), columns=sensor_df.columns)\n",
    "            \n",
    "            # put in the same order the columns of the sensors\n",
    "            X_sensor = X_sensor[X_columns]\n",
    "            \n",
    "            X_sensor['DateTime'] = datetime_col\n",
    "            cluster_0_sensor_dict[sensor_name] = X_sensor\n",
    "    elif cluster_name == 'cluster_1':\n",
    "        for sensor_name in cluster_1_sensor_dict.keys():\n",
    "            sensor_df = cluster_1_sensor_dict[sensor_name].copy()\n",
    "            datetime_col = sensor_df['DateTime']\n",
    "            sensor_df.drop(columns=['DateTime'], inplace=True)\n",
    "            \n",
    "            X_sensor = pd.DataFrame(scaler.fit_transform(sensor_df), columns=sensor_df.columns)\n",
    "            \n",
    "            # put in the same order the columns of the sensors\n",
    "            X_sensor = X_sensor[X_columns]\n",
    "            \n",
    "            X_sensor['DateTime'] = datetime_col\n",
    "            cluster_1_sensor_dict[sensor_name] = X_sensor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Different models will be used:\n",
    "\n",
    "- PLS\n",
    "- SVR\n",
    "- QRNNNN\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "for cluster_name in clusters.keys():\n",
    "    X, y = clusters[cluster_name]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clusters[cluster_name] = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS\n",
    "\n",
    "The Partial Least Squares regression (PLS) is a method which reduces the variables, used to predict, to a smaller set of predictors. These predictors are then used to perform a regression.\n",
    "\n",
    "It projects the predictors (independent variables) and the response variable (dependent variable) into a new space that maximizes the covariance between them. The procedure identifies components (latent variables) that explain the most variance in the predictors while also being predictive of the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the parameters of the model\n",
    "pls = PLSRegression()\n",
    "print(pls.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_pls_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_components = params[\"n_components\"]\n",
    "    tol = params[\"tol\"]\n",
    "\n",
    "    model = PLSRegression(\n",
    "        n_components=n_components,\n",
    "        tol=tol,\n",
    "        scale=False,\n",
    "        max_iter=1000,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_components\": trial.suggest_int(\"n_components\", 2, X_cv.shape[1]),\n",
    "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-1),\n",
    "        \n",
    "    }\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_pls_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    X_train, X_test, y_train, y_test = clusters[cluster_name]\n",
    "    \n",
    "    if os.path.exists(f\"supply_points_sqlites/PLS_{cluster_name}.sqlite3\"):\n",
    "        \n",
    "        study = optuna.load_study(\n",
    "            study_name=f\"Hyperparameter Tuning - PLS_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/PLS_{cluster_name}.sqlite3\",\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            study_name=f\"Hyperparameter Tuning - PLS_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/PLS_{cluster_name}.sqlite3\",\n",
    "            direction=\"minimize\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "    \n",
    "        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "    \n",
    "    pls_studies[cluster_name] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVR from sklearn\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the parameters of the model\n",
    "svr = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = [\n",
    "    \"linear\",\n",
    "    \"rbf\",\n",
    "    \"sigmoid\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_svr_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    kernel = params[\"kernel\"]\n",
    "    C = params[\"C\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    gamma = params[\"gamma\"]\n",
    "\n",
    "    model = SVR(\n",
    "        kernel=kernel,\n",
    "        C=C,\n",
    "        epsilon=epsilon,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", kernel),\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-6, 1, log=True),\n",
    "        \"epsilon\": trial.suggest_float(\"epsilon\", 1e-6, 1, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-6, 1, log=True),\n",
    "        \n",
    "    }\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_svr_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    X_train, X_test, y_train, y_test = clusters[cluster_name]\n",
    "    if os.path.exists(f\"supply_points_sqlites/SVR_{cluster_name}.sqlite3\"):\n",
    "        \n",
    "        study = optuna.load_study(\n",
    "            study_name=f\"Hyperparameter Tuning - SVR_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/SVR_{cluster_name}.sqlite3\",\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            study_name=f\"Hyperparameter Tuning - SVR_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/SVR_{cluster_name}.sqlite3\",\n",
    "            direction=\"minimize\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "    \n",
    "        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "    \n",
    "    svr_studies[cluster_name] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantnn.qrnn import QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.linspace(0.01, 0.99, 99)\n",
    "\n",
    "def fit_and_validate_qrnn_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index].to_numpy(), X.iloc[val_index].to_numpy()\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_layers = params[\"n_layers\"]\n",
    "    n_units = params[\"n_units\"]\n",
    "    activation = params[\"activation\"]\n",
    "\n",
    "    model = QRNN(\n",
    "        n_inputs=X_tr.shape[1],\n",
    "        quantiles=quantiles,\n",
    "        model=(n_layers, n_units, activation),\n",
    "    )\n",
    "    \n",
    "    n_epochs = 50\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    model.train(\n",
    "        training_data=(np.array(X_tr), np.array(y_tr)),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        n_epochs=n_epochs,\n",
    "        device=\"cpu\",\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        logger=None,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    \n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred.mean(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    \"elu\",\n",
    "    \"hardshrink\",\n",
    "    \"hardtanh\",\n",
    "    \"prelu\",\n",
    "    \"relu\",\n",
    "    \"selu\",\n",
    "    \"celu\",\n",
    "    \"sigmoid\",\n",
    "    \"softplus\",\n",
    "    \"softmin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 1, 3),\n",
    "        \"n_units\": trial.suggest_int(\"n_units\", 32, 512, log=True),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", activations),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16]),\n",
    "    }\n",
    "\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_qrnn_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrnn_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    X_train, X_test, y_train, y_test = clusters[cluster_name]\n",
    "    if os.path.exists(f\"supply_points_sqlites/QRNN_{cluster_name}.sqlite3\"):\n",
    "        \n",
    "        study = optuna.load_study(\n",
    "            study_name=f\"Hyperparameter Tuning - QRNN_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/QRNN_{cluster_name}.sqlite3\",\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            study_name=f\"Hyperparameter Tuning - QRNN_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/QRNN_{cluster_name}.sqlite3\",\n",
    "            direction=\"minimize\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "    \n",
    "        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "    \n",
    "    qrnn_studies[cluster_name] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_xgb_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = XGBRegressor(random_state=42, **params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    eta = trial.suggest_float(\"eta\", 1e-5, 1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1, log=True)\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", 1e-5, 1, log=True\n",
    "    )\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n",
    "    updater = trial.suggest_categorical(\n",
    "        \"updater\", [\"shotgun\", \"coord_descent\"]\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": eta,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"updater\": updater,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_xgb_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            params,\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    X_train, X_test, y_train, y_test = clusters[cluster_name]\n",
    "    if os.path.exists(f\"supply_points_sqlites/XGB_{cluster_name}.sqlite3\"):\n",
    "        \n",
    "        study = optuna.load_study(\n",
    "            study_name=f\"Hyperparameter Tuning - XGB_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/XGB_{cluster_name}.sqlite3\",\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            study_name=f\"Hyperparameter Tuning - XGB_{cluster_name}\",\n",
    "            storage=f\"sqlite:///supply_points_sqlites/XGB_{cluster_name}.sqlite3\",\n",
    "            direction=\"minimize\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "    \n",
    "        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "    \n",
    "    xgb_studies[cluster_name] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "best_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    best_studies[cluster_name] = {\n",
    "        \"PLS\": pls_studies[cluster_name].best_trial,\n",
    "        \"SVR\": svr_studies[cluster_name].best_trial,\n",
    "        \"QRNN\": qrnn_studies[cluster_name].best_trial,\n",
    "        \"XGB\": xgb_studies[cluster_name].best_trial,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    columns=best_studies[\"cluster_0\"].keys(),\n",
    "    index=list(best_studies.keys()),\n",
    ")\n",
    "\n",
    "for cluster, studies in best_studies.items():\n",
    "    for model, study in studies.items():\n",
    "        comparison_df.loc[cluster, model] = np.round(study.value, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_eval = {}\n",
    "cluster_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    medians = []\n",
    "    lower = []\n",
    "    upper = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "    \n",
    "        qrnn_best_trial = qrnn_studies[cluster_name].best_trial\n",
    "        X_train, X_test, y_train, y_test = clusters[cluster_name]\n",
    "\n",
    "        \n",
    "        qrnn = QRNN(\n",
    "            n_inputs=X_train.shape[1],\n",
    "            quantiles=[0.025, 0.5, 0.975],\n",
    "            model=(\n",
    "                qrnn_best_trial.params[\"n_layers\"],\n",
    "                qrnn_best_trial.params[\"n_units\"],\n",
    "                qrnn_best_trial.params[\"activation\"],\n",
    "            ),\n",
    "        )\n",
    "        n_epochs = 50\n",
    "        optimizer = torch.optim.AdamW(qrnn.model.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "        qrnn.train(\n",
    "            training_data=(X_train.to_numpy(), np.array(y_train)),\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            n_epochs=n_epochs,\n",
    "            device=\"cpu\",\n",
    "            batch_size=qrnn_best_trial.params[\"batch_size\"],\n",
    "            logger=None,\n",
    "        )\n",
    "        \n",
    "        cluster_models[cluster_name] = qrnn\n",
    "        \n",
    "        y_test_median = qrnn.predict(X_test.to_numpy())[:, 1]\n",
    "        y_test_lower = qrnn.predict(X_test.to_numpy())[:, 0]\n",
    "        y_test_upper = qrnn.predict(X_test.to_numpy())[:, 2]\n",
    "        \n",
    "        \n",
    "        \n",
    "        medians.append(y_test_median)\n",
    "        lower.append(y_test_lower)\n",
    "        upper.append(y_test_upper)\n",
    "\n",
    "    clusters_eval[cluster_name] = {\n",
    "        \"y_test\": y_test,\n",
    "        \"y_test_median\": np.mean(medians, axis=0),\n",
    "        \"y_test_lower\": np.mean(lower, axis=0),\n",
    "        \"y_test_upper\": np.mean(upper, axis=0),\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_name in clusters_eval.keys():\n",
    "    y_test = clusters_eval[cluster_name][\"y_test\"]\n",
    "    y_test_median = clusters_eval[cluster_name][\"y_test_median\"]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(y_test, y_test_median, \"o\")\n",
    "    plt.plot([0, 14], [0, 14], \"--\")\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    \n",
    "    plt.title(f\"{cluster_name} - True vs Predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the time series of the predictions\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    y_test = clusters_eval[cluster_name][\"y_test\"]\n",
    "    y_pred_qrnn_median = clusters_eval[cluster_name][\"y_test_median\"]\n",
    "    y_pred_qrnn_lower = clusters_eval[cluster_name][\"y_test_lower\"]\n",
    "    y_pred_qrnn_upper = clusters_eval[cluster_name][\"y_test_upper\"]\n",
    "    \n",
    "    grab_df_test = grab_df.iloc[y_test.index]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=grab_df_test['DateTime'],\n",
    "            y=y_test,\n",
    "            mode=\"markers\",\n",
    "            name=\"True TTHMs\",\n",
    "            line=dict(color=\"black\"),\n",
    "            marker=dict(size=10),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=grab_df_test['DateTime'],\n",
    "            y=y_pred_qrnn_median,\n",
    "            mode=\"markers\",\n",
    "            name=\"Predicted TTHMs (95% PI)\",\n",
    "            line=dict(color=\"green\"),\n",
    "            marker=dict(size=10),\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                symmetric=False,\n",
    "                array=y_pred_qrnn_upper,\n",
    "                arrayminus=y_pred_qrnn_lower,\n",
    "                thickness=2,\n",
    "                width=5,\n",
    "                color=\"green\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    cluster_index = int(cluster_name.split(\"_\")[-1]) + 1\n",
    "\n",
    "    # get the legend inside the plot\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        ),\n",
    "        margin=dict(l=10, r=10, t=30, b=10),\n",
    "        title=f\"Cluster {cluster_index - 1} - QRNN Predictions\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Time\")\n",
    "    fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "    \n",
    "    fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "    # update the overall font\n",
    "    fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(cluster_0_sensor_dict.keys()),\n",
    "    cols=1,\n",
    "    subplot_titles=list(cluster_0_sensor_dict.keys()),\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    x_title=\"Time\",\n",
    "    y_title=\"TTHMs (µg/L)\",\n",
    ")\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"red\",\n",
    "    \"green\",\n",
    "    \"purple\",\n",
    "    \"orange\",\n",
    "]\n",
    "\n",
    "show_true_TTHMs = True\n",
    "\n",
    "for index, supply_point_name in enumerate(cluster_0_sensor_dict.keys()):\n",
    "    \n",
    "    sensor_df = cluster_0_sensor_dict[supply_point_name].copy()\n",
    "    \n",
    "    supply_points_grab = grab_df[grab_df['Code'] == supply_point_name].copy()\n",
    "    \n",
    "    model = cluster_models[\"cluster_0\"]\n",
    "    \n",
    "    y_pred_median = model.predict(sensor_df[sensor_df.columns.difference([\"DateTime\"])].to_numpy())[:, 1]\n",
    "    y_pred_lower = model.predict(sensor_df[sensor_df.columns.difference([\"DateTime\"])].to_numpy())[:, 0]\n",
    "    y_pred_upper = model.predict(sensor_df[sensor_df.columns.difference([\"DateTime\"])].to_numpy())[:, 2]\n",
    "    \n",
    "    sensor_df['DateTime'] = pd.to_datetime(sensor_df['DateTime'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sensor_df['DateTime'],\n",
    "            y=y_pred_median,\n",
    "            mode=\"lines\",\n",
    "            name=\"Predicted TTHMs (95% PI)\",\n",
    "            line=dict(color=colors[index]),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sensor_df['DateTime'],\n",
    "            y=y_pred_lower,\n",
    "            name=\"Predicted TTHMs (95% PI) Lower\",\n",
    "            line=dict(width=0),\n",
    "            mode='lines',\n",
    "            fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "            fill='tonexty',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sensor_df['DateTime'],\n",
    "            y=y_pred_upper,\n",
    "            mode=\"lines\",\n",
    "            name=\"Predicted TTHMs (95% PI) Upper\",\n",
    "            line=dict(width=0),\n",
    "            fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "            fill='tonexty',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=supply_points_grab['DateTime'],\n",
    "            y=supply_points_grab['TTHMs'],\n",
    "            mode=\"markers\",\n",
    "            name=\"True TTHMs\",\n",
    "            line=dict(color=\"black\"),\n",
    "            marker=dict(size=10),\n",
    "            showlegend=show_true_TTHMs,\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1\n",
    "    )\n",
    "    \n",
    "    show_true_TTHMs = False\n",
    "    \n",
    "    # Update y-axis range for each subplot\n",
    "    fig.update_yaxes(range=[0, 15], row=index + 1, col=1)\n",
    "    \n",
    "fig.update_layout(\n",
    "    font=dict(family=\"Arial\", size=18),  # maintain font consistency\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    margin=dict(l=10, r=10, t=30, b=60),\n",
    "    legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        ),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# fig.write_image(\n",
    "#     \"cluster_0_sensor.png\",\n",
    "#     scale=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for the cluster 1\n",
    "fig = make_subplots(\n",
    "    rows=len(cluster_1_sensor_dict.keys()),\n",
    "    cols=1,\n",
    "    subplot_titles=list(cluster_1_sensor_dict.keys()),\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    x_title=\"Time\",\n",
    "    y_title=\"TTHMs (µg/L)\",\n",
    ")\n",
    "\n",
    "colors = [\n",
    "    \"brown\",\n",
    "    \"olive\",\n",
    "    \"teal\",\n",
    "    \"navy\",\n",
    "    \"pink\",\n",
    "]\n",
    "\n",
    "show_true_TTHMs = True\n",
    "\n",
    "for index, supply_point_name in enumerate(cluster_1_sensor_dict.keys()):\n",
    "    \n",
    "    sensor_df = cluster_1_sensor_dict[supply_point_name].copy()\n",
    "    \n",
    "    supply_points_grab = grab_df[grab_df['Code'] == supply_point_name].copy()\n",
    "    \n",
    "    model = cluster_models[\"cluster_1\"]\n",
    "    \n",
    "    y_pred_median = model.predict(sensor_df[sensor_df.columns.difference([\"DateTime\"])].to_numpy())[:, 1]\n",
    "    y_pred_lower = model.predict(sensor_df[sensor_df.columns.difference([\"DateTime\"])].to_numpy())[:, 0]\n",
    "    y_pred_upper = model.predict(sensor_df[sensor_df.columns.difference([\"DateTime\"])].to_numpy())[:, 2]\n",
    "    \n",
    "    sensor_df['DateTime'] = pd.to_datetime(sensor_df['DateTime'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sensor_df['DateTime'],\n",
    "            y=y_pred_median,\n",
    "            mode=\"lines\",\n",
    "            name=\"Predicted TTHMs (95% PI)\",\n",
    "            line=dict(color=colors[index]),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sensor_df['DateTime'],\n",
    "            y=y_pred_lower,\n",
    "            name=\"Predicted TTHMs (95% PI) Lower\",\n",
    "            line=dict(width=0),\n",
    "            mode='lines',\n",
    "            fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "            fill='tonexty',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sensor_df['DateTime'],\n",
    "            y=y_pred_upper,\n",
    "            mode=\"lines\",\n",
    "            name=\"Predicted TTHMs (95% PI) Upper\",\n",
    "            line=dict(width=0),\n",
    "            fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "            fill='tonexty',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=supply_points_grab['DateTime'],\n",
    "            y=supply_points_grab['TTHMs'],\n",
    "            mode=\"markers\",\n",
    "            name=\"True TTHMs\",\n",
    "            line=dict(color=\"black\"),\n",
    "            marker=dict(size=10),\n",
    "            showlegend=show_true_TTHMs,\n",
    "        ),\n",
    "        row=index + 1,\n",
    "        col=1\n",
    "    )\n",
    "    \n",
    "    show_true_TTHMs = False\n",
    "    \n",
    "fig.update_layout(\n",
    "    font=dict(family=\"Arial\", size=18),  # maintain font consistency\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    margin=dict(l=10, r=10, t=30, b=60),\n",
    "    legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        ),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# fig.write_image(\n",
    "#     \"cluster_1_sensor.png\",\n",
    "#     scale=3,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-t4-2-cs2-supply-points-VbhzC-WI-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
